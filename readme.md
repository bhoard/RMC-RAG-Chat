# Simple RAG Pipeline

A straightforward, easy-to-understand RAG (Retrieval-Augmented Generation) pipeline built with Python and SQLite. Perfect for learning or deploying on a small VPS.

## ğŸ¯ Project Goals

- **Simple & Educational**: Written with junior developers in mind
- **Resource Efficient**: Runs on modest hardware (tested on 4-core i5, 16GB RAM)
- **Self-Contained**: Uses SQLite (no external database servers needed)
- **Transparent**: Each pipeline stage is a separate, well-commented script

## ğŸ“‹ Requirements

- Ubuntu 24.04 LTS (or similar Linux distribution)
- Python 3.10+
- 4GB+ RAM (16GB recommended)
- 20GB+ storage (100GB recommended for larger sites)

## ğŸš€ Quick Start

### 1. Initial Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/simple-rag-pipeline.git
cd simple-rag-pipeline

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
make setup
```

### 2. Configure Your Site

Edit `settings.yaml` and update the sitemap URL:

```yaml
site:
  sitemap_url: "https://your-site.com/sitemap.xml"
```

### 3. Run the Pipeline

```bash
# Fetch URLs from sitemap
make stage1

# Check what was found
make stats
```

## ğŸ“ Project Structure

```
simple-rag-pipeline/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ LICENSE                   # MIT License
â”œâ”€â”€ CONTRIBUTING.md           # Contribution guidelines
â”œâ”€â”€ settings.yaml             # Configuration file
â”œâ”€â”€ Makefile                  # Pipeline management commands
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ 1_fetch_sitemap.py       # Stage 1: Fetch URLs from sitemap
â”œâ”€â”€ 2_crawl_pages.py         # Stage 2: Crawl and extract content
â”œâ”€â”€ recrawl.py               # Utility: Force recrawl URLs
â”œâ”€â”€ 3_process_content.py     # Stage 3: Chunk and process text
â”œâ”€â”€ 4_generate_embeddings.py # Stage 4: Create vector embeddings (coming soon)
â”œâ”€â”€ crawl_ledger.db          # SQLite: URLs and raw content
â”œâ”€â”€ chunks.db                # SQLite: Processed chunks
â””â”€â”€ vector_store.db          # SQLite: Vector embeddings (coming soon)
```

## ğŸ”§ Available Commands

Run `make` or `make menu` to see all available commands:

- `make setup` - Install dependencies
- `make stage1` - Fetch sitemap URLs
- `make stage2` - Crawl pages with Crawl4AI
- `make stage3` - Process content
- `make stage4` - Generate embeddings (coming soon)
- `make stats` - Show database statistics
- `make show-content` - Preview crawled content
- `make show-chunks` - Preview chunks
- `make inspect-db` - Open SQLite shell
- `make recrawl URL='<pattern>'` - Force recrawl specific URLs
- `make clean` - Remove all databases

## ğŸ—„ï¸ Database Schema

### crawl_ledger.db

**pages table**:
- `url` (TEXT, PRIMARY KEY) - The page URL
- `cleaned_text` (TEXT) - Extracted and cleaned markdown content
- `date_success` (TEXT) - Last successful crawl timestamp
- `date_fail` (TEXT) - Last failed crawl timestamp
- `fail_count` (INTEGER) - Number of consecutive failures

### chunks.db

**chunks table**:
- `chunk_id` (INTEGER, PRIMARY KEY) - Auto-incrementing ID
- `url` (TEXT) - Source URL
- `chunk_index` (INTEGER) - Position in document (0, 1, 2...)
- `chunk_text` (TEXT) - The chunk content
- `chunk_size` (INTEGER) - Length in characters
- `token_count` (INTEGER) - Approximate token count
- `heading_context` (TEXT) - Markdown heading this chunk falls under
- `created_at` (TEXT) - When chunk was created

## ğŸ› ï¸ Pipeline Stages

### Stage 1: Fetch Sitemap âœ…
Fetches your sitemap.xml and extracts all URLs into the database.

```bash
make stage1
```

### Stage 2: Crawl Pages âœ…
Uses Crawl4AI to fetch and clean content from each URL. Stores markdown content in database.

Features:
- Configurable batch sizes
- Smart prioritization (never crawled â†’ failed â†’ stale)
- URL ignore patterns
- Automatic retry logic
- Periodic recrawling

```bash
# Crawl pages (uses batch_size from settings.yaml)
make stage2

# Force recrawl specific URLs
make recrawl URL='/blog/*'
make recrawl URL='https://example.com/specific-page'
```

### Stage 3: Process Content âœ…
Chunks text into manageable pieces for embedding. Features intelligent markdown-aware chunking.

Features:
- Configurable chunk sizes
- Respects sentence and paragraph boundaries
- Preserves markdown heading context
- Tracks staleness for re-processing
- Separate chunks database

```bash
# Process all crawled pages into chunks
make stage3

# View chunk statistics
make stats

# Preview sample chunks
make show-chunks
```

### Stage 4: Generate Embeddings âœ…
Creates vector embeddings for semantic search using sentence-transformers.

Features:
- Configurable models (default: all-mpnet-base-v2, 768-dim)
- Smart batch processing
- Tracks embedding staleness
- Automatic re-embedding when chunks or model changes
- Separate embeddings database

```bash
# Generate embeddings (uses batch_size from settings.yaml)
make stage4

# View statistics
make stats

# Preview sample embeddings
make show-embeddings

# To switch models, edit settings.yaml:
# embeddings:
#   model: "all-MiniLM-L6-v2"
#   dimension: 384
# Then run: make stage4
```

## ğŸ¤ Contributing

Contributions are welcome! This project aims to be educational and accessible. When contributing:

1. Keep code simple and well-commented
2. Write as if explaining to a junior developer
3. Test on modest hardware when possible
4. Update documentation for any new features

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- Built with [Crawl4AI](https://github.com/unclecode/crawl4ai)
- Uses SQLite for simple, portable storage
- Inspired by the need for accessible RAG implementations

## ğŸ“§ Support

Found a bug? Have a question? [Open an issue](https://github.com/yourusername/simple-rag-pipeline/issues)

---

**Status**: ğŸš§ Stages 1-3 Complete | Stage 4 In Development
